{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook for Acceptance Test Campaign related to LSST Science Pipelines Release 19.0.0\n",
    "\n",
    "This test will be executed on the LSST Science Platform Notebook Aspect, initialized with Science Pipelines release `r19-0-0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case LVV-T62: Verify implementation of Provide PSF for Coadded Images\n",
    "Verify that all coadd images produced by the DRP pipelines include a model from which an image of the PSF at any point on the coadd can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the version of the Science Pipelines is v19_0_0:\n",
    "! echo $HOSTNAME\n",
    "! eups list -s | grep lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.daf.persistence as dafPersist\n",
    "import lsst.geom as geom\n",
    "from lsst.afw.image import Exposure, Image, PARENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from astropy.visualization import (MinMaxInterval, AsinhStretch, ZScaleInterval, LogStretch, LinearStretch,\n",
    "                                   ImageNormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting defaults\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "zscale = ZScaleInterval()\n",
    "\n",
    "# Set up some plotting defaults:\n",
    "plt.rcParams.update({'figure.figsize' : (12, 8)})\n",
    "plt.rcParams.update({'font.size' : 10})\n",
    "plt.rcParams.update({'axes.linewidth' : 3})\n",
    "plt.rcParams.update({'axes.labelweight' : 3})\n",
    "plt.rcParams.update({'axes.titleweight' : 3})\n",
    "plt.rcParams.update({'ytick.major.width' : 3})\n",
    "plt.rcParams.update({'ytick.minor.width' : 2})\n",
    "plt.rcParams.update({'ytick.major.size' : 8})\n",
    "plt.rcParams.update({'ytick.minor.size' : 5})\n",
    "plt.rcParams.update({'xtick.major.size' : 8})\n",
    "plt.rcParams.update({'xtick.minor.size' : 5})\n",
    "plt.rcParams.update({'xtick.major.width' : 3})\n",
    "plt.rcParams.update({'xtick.minor.width' : 2})\n",
    "plt.rcParams.update({'xtick.direction' : 'in'})\n",
    "plt.rcParams.update({'ytick.direction' : 'in'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use HSC-RC2, as processed using `w_2019_46`, which is the pipelines version that was used to create `v19_0_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output repo is tagged with the Jira ticket number \"DM-22223\":\n",
    "\n",
    "rc2_repo = '/datasets/hsc/repo/rerun/RC/w_2019_46/DM-22223'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the butler repo:\n",
    "butler = dafPersist.Butler(rc2_repo)\n",
    "\n",
    "# Make a glob of the files in the repo, so we can parse this to get tract/patch IDs:\n",
    "infiles = glob.glob(rc2_repo+'/deepCoadd-results/HSC-*/*/*/calexp-*.fits')\n",
    "print(len(infiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make arrays of the tract, patch numbers by parsing the filenamesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_array = []\n",
    "patch_array = []\n",
    "\n",
    "# Each line of \"infiles\" looks like this (infile[0]):\n",
    "# '/datasets/hsc/repo/rerun/RC/w_2019_46/DM-22223/deepCoadd-results/HSC-R/9615/8,8/calexp-HSC-R-9615-8,8.fits'\n",
    "\n",
    "# Split on the '/', then parse the resulting array. Here's the result of infiles[0].split('/'):\n",
    "\n",
    "#['',\n",
    "# 'datasets',\n",
    "# 'hsc',\n",
    "# 'repo',\n",
    "# 'rerun',\n",
    "# 'RC',\n",
    "# 'w_2019_46',\n",
    "# 'DM-22223',\n",
    "# 'deepCoadd',\n",
    "# 'HSC-R',\n",
    "# '9615',\n",
    "# '8,8',\n",
    "# 'warp-HSC-R-9615-8,8-23902.fits']\n",
    "\n",
    "# So the tract number is third from the end, and patch is second from the end.\n",
    "\n",
    "for ii in range(0, len(infiles)):\n",
    "    parts = infiles[ii].split('/')\n",
    "    tract_array.append(int(parts[-3]))\n",
    "    patch_array.append(parts[-2])\n",
    "\n",
    "print('Found %i patches'%(len(patch_array)))\n",
    "    \n",
    "# Assemble in pandas data frame    \n",
    "data = {'tract': tract_array,\n",
    "        'patch': patch_array}\n",
    "df_tract_patch = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tract_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tract names that are available:\n",
    "print('The repo contains the following tracts: ',df_tract_patch.tract.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some tract/patch combinations from these at random:\n",
    "numpatches = 6 # 12\n",
    "patch_sel = df_tract_patch.sample(numpatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CCDs number from 0-111. Randomly select a CCD for each of the visits:\n",
    "#ccdnums = np.random.randint(112, size=numpatches)\n",
    "#\n",
    "## CCD 9 is bad, so generate a new array until we have no \"9\" values in it:\n",
    "#exclude_ccds = [9]\n",
    "#while ccdnums.any() in exclude_ccds:\n",
    "#    ccdnums = np.random.randint(112, size=numvisits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calexps = [] ### The calexps are too big, causing the kernel to run out of memory and die. :(\n",
    "datarefs = []\n",
    "\n",
    "for row, pch in patch_sel.iterrows():\n",
    "    # print(vis)\n",
    "    # Note: we are using only the 'HSC-R' filter for this, but other filters could be selected:\n",
    "    dataref = {'tract':int(pch.tract), 'patch':pch.patch, 'filter':'HSC-R'}\n",
    "    # Sometimes CCDs fail, so test whether the dataset exists before grabbing it:\n",
    "    if butler.datasetExists('deepCoadd_calexp', dataId = dataref):\n",
    "#        calexp = butler.get('deepCoadd_calexp', dataId = dataref)\n",
    "#        calexps.append(calexp)\n",
    "        datarefs.append(dataref)\n",
    "    else:\n",
    "        print('dataref: '+str(dataref)+' does not exist. Skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datarefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a simple way to confirm that a given `calexp` image has a PSF is via the method: `calexp.hasPsf()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each of the selected tract/patch combinations, pick a random (X,Y) coordinate, extract the PSF, and plot an image of the PSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3 # 4\n",
    "nrows = numpatches/ncols # 4 plots per row\n",
    "f, ax = plt.subplots(nrows=int(np.ceil(nrows)), ncols=int(ncols), sharey=True, sharex=True, figsize=(12,10))\n",
    "axnum = 0\n",
    "axs = ax.flatten()\n",
    "\n",
    "for dataref in datarefs:\n",
    "#    print(c.hasPsf())\n",
    "    c = butler.get('deepCoadd_calexp', dataId = dataref)\n",
    "    psf = c.getPsf()\n",
    "    xsize = c.getDimensions().getX()\n",
    "    ysize = c.getDimensions().getY()\n",
    "    x0 = c.getX0()\n",
    "    y0 = c.getY0()\n",
    "\n",
    "    # Select a random point on the image and extract the PSF at that point:\n",
    "    xpt = x0+random.random()*xsize\n",
    "    ypt = y0+random.random()*ysize\n",
    "\n",
    "    noimg_flag = False\n",
    "    try:\n",
    "        psfimage = psf.computeImage(geom.PointD(xpt, ypt))\n",
    "    except:\n",
    "        print('No images at the requested point.')\n",
    "        noimg_flag = True\n",
    "        \n",
    "    if not noimg_flag:\n",
    "        \n",
    "        img = psfimage.array\n",
    "\n",
    "        # Create an ImageNormalize object\n",
    "        norm = ImageNormalize(img, interval=MinMaxInterval(),\n",
    "                              stretch=LogStretch())\n",
    "\n",
    "#    axs[axnum].set_xlim(psfimage.getX0(),psfimage.getX0()+psfimage.getDimensions()[0])\n",
    "#    axs[axnum].set_ylim(psfimage.getY0(),psfimage.getY0()+psfimage.getDimensions()[1])\n",
    "        axs[axnum].set_title('patch: '+patch_sel.iloc[axnum].patch+';\\n (x,y)=('+str(round(xpt,1))+', '+str(round(ypt,1))+')')\n",
    "        axs[axnum].imshow(img, norm=norm, origin='lower')\n",
    "\n",
    "    axnum += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now confirm that the PSF is reasonably well-matched to stellar images by selecting a bright star in each image and subtracting the PSF at the star's position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a bright star that was used to fit the PSF, and one that was used to calibrate the photometry:\n",
    "src = butler.get('deepCoadd_forced_src', dataId=dataref)\n",
    "ref = butler.get('deepCoadd_ref', dataId=dataref)\n",
    "\n",
    "is_primary = (ref['detect_isPrimary'] == True) & (np.isfinite(src.getPsfInstFlux()))\n",
    "deblended = (src[\"deblend_nChild\"] == 0)\n",
    "ptsource = (src['base_ClassificationExtendedness_flag'] == False)\n",
    "\n",
    "# Make a copy of the star selection that is contiguous in memory:\n",
    "stars = src[is_primary & deblended & ptsource].copy(deep=True)\n",
    "stars_flux = stars.getPsfInstFlux()\n",
    "\n",
    "# Pick stars between the 85th and 90th percentiles in PSF flux:\n",
    "p85 = np.percentile(stars_flux, 85)\n",
    "p90 = np.percentile(stars_flux, 90)\n",
    "keepers = (stars_flux > p85) & (stars_flux < p90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_sel = random.choice(stars[keepers])\n",
    "radec_star_sel = star_sel.getCoord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs = c.getWcs()\n",
    "\n",
    "cutoutSize = geom.ExtentI(100, 100) # size of cutout in pixels\n",
    "xy = geom.Point2I(wcs.skyToPixel(radec_star_sel)) # central XY coordinate of our star's RA, Dec position\n",
    "\n",
    "# Create the bounding box:\n",
    "bbox = geom.Box2I(xy - cutoutSize//2, cutoutSize)\n",
    "\n",
    "# Full patch image\n",
    "#image = butler.get('calexp', immediate=True, dataId=dataid)\n",
    "# Because an entire tract shares a WCS, the corner of the patch (or cutout) isn't necessarily at (X,Y)=(0,0). Get the XY0 pixel values:\n",
    "xy0 = c.getXY0() \n",
    "\n",
    "# Postage stamp image only, using the bbox defined above:\n",
    "cutout_image = butler.get('deepCoadd_calexp_sub', bbox=bbox, immediate=True, dataId=dataref).getMaskedImage()\n",
    "# Because an entire tract shares a WCS, the corner of the patch (or cutout) isn't necessarily at (X,Y)=(0,0). Get the XY0 pixel values:\n",
    "xy0_cutout = cutout_image.getXY0() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf = c.getPsf()\n",
    "model = Exposure(cutout_image.getBBox(), dtype=np.float32)\n",
    "model.setPsf(psf)\n",
    "psfImage = psf.computeImage(star_sel.getCentroid())\n",
    "flux = star_sel['base_PsfFlux_instFlux']-(star_sel['base_PsfFlux_area']*star_sel['base_LocalBackground_instFlux'])\n",
    "psfBBox = psfImage.getBBox()\n",
    "model.image[psfBBox, PARENT].scaledPlus(flux, psfImage.convertF())\n",
    "residuals = cutout_image.clone()\n",
    "residuals.image -= model.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=3, sharey=True, sharex=True, figsize=(12,6))\n",
    "axs = ax.flatten()\n",
    "\n",
    "vmin0, vmax0 = zscale.get_limits(cutout_image.image.array)\n",
    "# Get the dimensions of the image so we can set plot limits\n",
    "imsize = cutout_image.getDimensions()\n",
    "axs[0].imshow(cutout_image.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "# Set the plot range to the dimensions:\n",
    "axs[0].set_xlim(0,imsize[0])\n",
    "axs[0].set_ylim(0,imsize[1])\n",
    "axs[0].set_title('Original image')\n",
    "axs[0].scatter(xy.getX()-xy0_cutout.getX(), xy.getY()-xy0_cutout.getY(), color='none', edgecolor='magenta', s=2000, linewidth=5)\n",
    "\n",
    "# Get the dimensions of the image so we can set plot limits\n",
    "imsize = model.image.getDimensions()\n",
    "axs[1].imshow(model.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "# Set the plot range to the dimensions:\n",
    "axs[1].set_xlim(0,imsize[0])\n",
    "axs[1].set_ylim(0,imsize[1])\n",
    "axs[1].set_title('Scaled PSF')\n",
    "\n",
    "# Get the dimensions of the image so we can set plot limits\n",
    "imsize = residuals.image.getDimensions()\n",
    "axs[2].imshow(residuals.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "# Set the plot range to the dimensions:\n",
    "axs[2].set_xlim(0,imsize[0])\n",
    "axs[2].set_ylim(0,imsize[1])\n",
    "axs[2].set_title('PSF subtracted')\n",
    "axs[2].scatter(xy.getX()-xy0_cutout.getX(), xy.getY()-xy0_cutout.getY(), color='none', edgecolor='magenta', s=2000, linewidth=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the images of some bright stars, and see how well the PSF matches the stellar image:\n",
    "For the images selected at random, pick (again, at random) a bright star and look at (a) its image, (b) an image of the scaled PSF at its position, and (c) the residuals after subtracting the scaled PSF from the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = numpatches # a star from each patch per row\n",
    "f, ax = plt.subplots(nrows=int(nrows), ncols=int(ncols), sharey=True, sharex=True, figsize=(5,12))\n",
    "axnum = 0\n",
    "axs = ax.flatten()\n",
    "plt.rcParams.update({'font.size' : 10})\n",
    "\n",
    "for dataref in datarefs:\n",
    "#    print(c.hasPsf())\n",
    "    c = butler.get('deepCoadd_calexp', dataId = dataref)\n",
    "\n",
    "    # Load the source catalog:\n",
    "    src = butler.get('deepCoadd_forced_src', dataId=dataref)\n",
    "    ref = butler.get('deepCoadd_ref', dataId=dataref)\n",
    "\n",
    "    is_primary = (ref['detect_isPrimary'] == True) & (np.isfinite(src.getPsfInstFlux()))\n",
    "    deblended = (src[\"deblend_nChild\"] == 0)\n",
    "    ptsource = (src['base_ClassificationExtendedness_flag'] == False)\n",
    "\n",
    "    # Make a copy of the star selection that is contiguous in memory:\n",
    "    stars = src[is_primary & deblended & ptsource].copy(deep=True)\n",
    "    stars_flux = stars.getPsfInstFlux()\n",
    "\n",
    "    # Pick stars between the 85th and 90th percentiles in PSF flux:\n",
    "    p85 = np.percentile(stars_flux, 85)\n",
    "    p90 = np.percentile(stars_flux, 90)\n",
    "    keepers = (stars_flux > p85) & (stars_flux < p90)\n",
    "\n",
    "    wcs = c.getWcs()\n",
    "    # Because an entire tract shares a WCS, the corner of the patch (or cutout) isn't necessarily at (X,Y)=(0,0). Get the XY0 pixel values:\n",
    "    xy0 = c.getXY0() \n",
    "\n",
    "    # size of image cutout in pixels\n",
    "    imsize = 100\n",
    "    cutoutSize = geom.ExtentI(imsize, imsize)\n",
    "    \n",
    "    calexp_dimen = c.getDimensions()\n",
    "\n",
    "#    # Pick one from the table at random, but require it to have coordinates\n",
    "#    #   such that the bounding box will not extend beyond the CCD's border:\n",
    "#    oksrc = False\n",
    "#    while not oksrc:\n",
    "#        star_sel = random.choice(stars[keepers])\n",
    "#        xy = star_sel.getCentroid()\n",
    "#        xpos = xy.getX()-xy0.getX()\n",
    "#        ypos = xy.getY()-xy0.getY()\n",
    "#        if (xpos > imsize/2.0) and (xpos < calexp_dimen[0]-imsize/2.0) and (ypos > imsize/2.0) and (ypos < calexp_dimen[1]-imsize/2.0):\n",
    "#            oksrc = True\n",
    "\n",
    "#    # star_sel = random.choice(stars[keepers])\n",
    "#    radec_star_sel = star_sel.getCoord()\n",
    "\n",
    "    # It's possible that this will pick a star with bounding box extending beyond the image. That's what the loop while loop above is\n",
    "    #    intended for. But it seemed to take forever, so something's clearly wrong...\n",
    "    star_sel = random.choice(stars[keepers])\n",
    "    xy = star_sel.getCentroid()\n",
    "\n",
    "    # Create the bounding box:\n",
    "    xy = geom.Point2I(xy)\n",
    "    bbox = geom.Box2I(xy - cutoutSize//2, cutoutSize)\n",
    "\n",
    "    # Postage stamp image only, using the bbox defined above:\n",
    "    cutout_image = butler.get('deepCoadd_calexp_sub', bbox=bbox, immediate=True, dataId=dataref).getMaskedImage()\n",
    "    # Because an entire tract shares a WCS, the corner of the patch (or cutout) isn't necessarily at (X,Y)=(0,0). Get the XY0 pixel values:\n",
    "    xy0_cutout = cutout_image.getXY0() \n",
    "\n",
    "    psf = c.getPsf()\n",
    "    model = Exposure(cutout_image.getBBox(), dtype=np.float32)\n",
    "    model.setPsf(psf)\n",
    "    psfImage = psf.computeImage(star_sel.getCentroid())\n",
    "    flux = star_sel['base_PsfFlux_instFlux']-(star_sel['base_PsfFlux_area']*star_sel['base_LocalBackground_instFlux'])\n",
    "    psfBBox = psfImage.getBBox()\n",
    "    model.image[psfBBox, PARENT].scaledPlus(flux, psfImage.convertF())\n",
    "    residuals = cutout_image.clone()\n",
    "    residuals.image -= model.image\n",
    "\n",
    "    vmin0, vmax0 = zscale.get_limits(cutout_image.image.array)\n",
    "    # Get the dimensions of the image so we can set plot limits\n",
    "    imsize = cutout_image.getDimensions()\n",
    "    axs[axnum].imshow(cutout_image.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "    # Set the plot range to the dimensions:\n",
    "    axs[axnum].set_xlim(0,imsize[0])\n",
    "    axs[axnum].set_ylim(0,imsize[1])\n",
    "    axs[axnum].set_title('Original image')\n",
    "    axs[axnum].get_xaxis().set_visible(False)\n",
    "    axs[axnum].get_yaxis().set_visible(False)\n",
    "\n",
    "    axnum += 1\n",
    "\n",
    "    # Get the dimensions of the image so we can set plot limits\n",
    "    imsize = model.image.getDimensions()\n",
    "    axs[axnum].imshow(model.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "    # Set the plot range to the dimensions:\n",
    "    axs[axnum].set_xlim(0,imsize[0])\n",
    "    axs[axnum].set_ylim(0,imsize[1])\n",
    "    axs[axnum].set_title('Scaled PSF')\n",
    "    axs[axnum].get_xaxis().set_visible(False)\n",
    "    axs[axnum].get_yaxis().set_visible(False)\n",
    "    \n",
    "    axnum += 1\n",
    "\n",
    "    # Get the dimensions of the image so we can set plot limits\n",
    "    imsize = residuals.image.getDimensions()\n",
    "    axs[axnum].imshow(residuals.image.array, vmin=vmin0, vmax=vmax0, cmap='binary')\n",
    "    # Set the plot range to the dimensions:\n",
    "    axs[axnum].set_xlim(0,imsize[0])\n",
    "    axs[axnum].set_ylim(0,imsize[1])\n",
    "    axs[axnum].set_title('PSF subtracted')\n",
    "    axs[axnum].get_xaxis().set_visible(False)\n",
    "    axs[axnum].get_yaxis().set_visible(False)\n",
    "\n",
    "    axnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the \"hasPsf\" method to check whether a larger number of the patch images have an associated PSF model:\n",
    "(Note: we use only a randomly-selected subset of images because it takes too long to extract the calexp for all images in the repository.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some tract/patch combinations from the repo at random:\n",
    "numpatches_boolcheck = 100\n",
    "patch_sel_boolcheck = df_tract_patch.sample(numpatches_boolcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarefs_boolcheck = []\n",
    "psfflags = []\n",
    "\n",
    "for row, pch in patch_sel_boolcheck.iterrows():\n",
    "    # print(vis)\n",
    "    # Note: we are using only the 'HSC-R' filter for this, but other filters could be selected:\n",
    "    dataref = {'tract':int(pch.tract), 'patch':pch.patch, 'filter':'HSC-R'}\n",
    "    # Sometimes CCDs fail, so test whether the dataset exists before grabbing it:\n",
    "    if butler.datasetExists('deepCoadd_calexp', dataId = dataref):\n",
    "        calexp = butler.get('deepCoadd_calexp', dataId = dataref)\n",
    "        psfflags.append(calexp.hasPsf())\n",
    "        datarefs.append(dataref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that _all_ images that we checked have an associated PSF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.all(psfflags)), 'FALSE: not all patches have an associated PSF model.'\n",
    "print('All patches have an associated PSF model: ',np.all(psfflags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
